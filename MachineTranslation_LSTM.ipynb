{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf5KrEb6vrkR"
      },
      "source": [
        "# Welcome to Colab!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 1: INSTALL REQUIRED LIBRARIES\n",
        "# ============================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import math\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load Spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "en_nlp = spacy.load('en_core_web_sm')\n",
        "de_nlp = spacy.load('de_core_news_sm')"
      ],
      "metadata": {
        "id": "C6SxRCUGW5p9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ed725c-f188-4523-b704-6692298676d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting de-core-news-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 2: LOAD DATASET AND CREATE 80/20 SPLIT\n",
        "# ============================================================\n",
        "from datasets import load_dataset # This line fixes your NameError\n",
        "\n",
        "# 1. Load the official Multi30k dataset\n",
        "dataset = load_dataset(\"bentrevett/multi30k\")\n",
        "\n",
        "# 2. Use ONLY the 'train' split (approx 29,000 rows)\n",
        "# We perform an 80/20 split on this portion only.\n",
        "# seed=42 ensures the shuffle is identical for consistency.\n",
        "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# 3. Assign the new splits\n",
        "train_raw = split_dataset[\"train\"]\n",
        "test_raw = split_dataset[\"test\"]\n",
        "\n",
        "# 4. Verification Print\n",
        "print(\"--- DATASET VERIFICATION ---\")\n",
        "print(f\"Total rows in original train split: {len(dataset['train'])}\")\n",
        "print(f\"Training Set (80%): {len(train_raw)}\")\n",
        "print(f\"Testing Set (20%): {len(test_raw)}\")\n",
        "print(\"----------------------------\")"
      ],
      "metadata": {
        "id": "FysITlD3Xhou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f00fc7-b6f1-4309-f286-998164ce66e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DATASET VERIFICATION ---\n",
            "Total rows in original train split: 29000\n",
            "Training Set (80%): 23200\n",
            "Testing Set (20%): 5800\n",
            "----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 3: Preprocessing (Tokenization & Vocab Building)\n",
        "# ============================================================\n",
        "import spacy\n",
        "import torch\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# DIRECT LOADING STRATEGY\n",
        "# This prevents the OSError by bypassing spaCy's shortcut system\n",
        "try:\n",
        "    import en_core_web_sm\n",
        "    import de_core_news_sm\n",
        "    en_nlp = en_core_web_sm.load()\n",
        "    de_nlp = de_core_news_sm.load()\n",
        "except ImportError:\n",
        "    # Fallback if the packages aren't recognized as modules yet\n",
        "    en_nlp = spacy.load('en_core_web_sm')\n",
        "    de_nlp = spacy.load('de_core_news_sm')\n",
        "\n",
        "def tokenize_en(text): return [tok.text.lower() for tok in en_nlp.tokenizer(text)]\n",
        "def tokenize_de(text): return [tok.text.lower() for tok in de_nlp.tokenizer(text)]\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, data, tokenize_fn, is_en=True):\n",
        "        self.itos = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
        "        self.stoi = {token: i for i, token in enumerate(self.itos)}\n",
        "        counter = Counter()\n",
        "        for item in data:\n",
        "            text = item['en'] if is_en else item['de']\n",
        "            counter.update(tokenize_fn(text))\n",
        "        for token, freq in counter.items():\n",
        "            if freq > 1: # Standard frequency threshold\n",
        "                self.stoi[token] = len(self.itos)\n",
        "                self.itos.append(token)\n",
        "    def __len__(self): return len(self.itos)\n",
        "    def encode(self, tokens): return [self.stoi.get(t, self.stoi['<unk>']) for t in tokens]\n",
        "    def decode(self, indices): return [self.itos[i] for i in indices]\n",
        "\n",
        "# Build Vocabularies from your 80% train split (23,200 sentences)\n",
        "# We assume train_raw and test_raw are already defined in your environment\n",
        "vocab_en = Vocab(train_raw, tokenize_en, is_en=True)\n",
        "vocab_de = Vocab(train_raw, tokenize_de, is_en=False)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_list, trg_list = [], []\n",
        "    for item in batch:\n",
        "        # Prepend <sos> and append <eos>\n",
        "        src_enc = [vocab_en.stoi['<sos>']] + vocab_en.encode(tokenize_en(item['en'])) + [vocab_en.stoi['<eos>']]\n",
        "        trg_enc = [vocab_de.stoi['<sos>']] + vocab_de.encode(tokenize_de(item['de'])) + [vocab_de.stoi['<eos>']]\n",
        "        src_list.append(torch.tensor(src_enc))\n",
        "        trg_list.append(torch.tensor(trg_enc))\n",
        "\n",
        "    # Pad sequences to match the longest sentence in the batch\n",
        "    return pad_sequence(src_list, padding_value=vocab_en.stoi['<pad>']).to(device), \\\n",
        "           pad_sequence(trg_list, padding_value=vocab_de.stoi['<pad>']).to(device)\n",
        "\n",
        "# Initialize DataLoaders\n",
        "train_loader = DataLoader(train_raw, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_raw, batch_size=32, collate_fn=collate_fn)\n",
        "\n",
        "print(\"--- BLOCK 3 SUCCESS ---\")\n",
        "print(f\"English Vocab Size: {len(vocab_en)}\")\n",
        "print(f\"German Vocab Size: {len(vocab_de)}\")"
      ],
      "metadata": {
        "id": "D2OizxbvXsPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a0531c-d241-41df-c15c-ad416b3fc4a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- BLOCK 3 SUCCESS ---\n",
            "English Vocab Size: 5314\n",
            "German Vocab Size: 6808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 4: LSTM MODEL DEFINITION (ENCODER-DECODER)\n",
        "# ============================================================\n",
        "class Encoder(nn.Module):\n",
        "    def init(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().init()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def init(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().init()\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def init(self, encoder, decoder, device):\n",
        "        super().init()\n",
        "        self.encoder, self.decoder, self.device = encoder, decoder, device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        trg_len, batch_size = trg.shape\n",
        "        outputs = torch.zeros(trg_len, batch_size, self.decoder.output_dim).to(self.device)\n",
        "        hidden, cell = self.encoder(src)\n",
        "        input = trg[0,:]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            input = trg[t] if random.random() < teacher_forcing_ratio else output.argmax(1)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "x0gC9YLBYDB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK 5: EVALUATION MATRICES (LOSS, PERPLEXITY, BLEU)\n",
        "# ============================================================\n",
        "# 1. Install sacrebleu if it's not already installed\n",
        "!pip install sacrebleu\n",
        "\n",
        "import sacrebleu\n",
        "import math\n",
        "import torch\n",
        "\n",
        "def get_metrics(model, loader, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    predictions, references = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in loader:\n",
        "            # Turn off teacher forcing (ratio=0) for true evaluation\n",
        "            output = model(src, trg, 0)\n",
        "\n",
        "            # Loss Calculation (Flatten output and target tensors)\n",
        "            # output: [trg_len, batch_size, output_dim] -> [ (trg_len-1)*batch_size, output_dim ]\n",
        "            # trg: [trg_len, batch_size] -> [ (trg_len-1)*batch_size ]\n",
        "            loss = criterion(output[1:].view(-1, output.shape[-1]), trg[1:].view(-1))\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Decode for BLEU\n",
        "            # Convert model outputs to token indices\n",
        "            preds = output.argmax(2).transpose(0, 1) # [batch_size, trg_len]\n",
        "            targets = trg.transpose(0, 1)\n",
        "\n",
        "            for i in range(preds.shape[0]):\n",
        "                p = vocab_de.decode(preds[i].tolist())\n",
        "                r = vocab_de.decode(targets[i].tolist())\n",
        "\n",
        "                pred_sent = \" \".join([t for t in p if t not in ['<sos>', '<eos>', '<pad>']])\n",
        "                ref_sent = \" \".join([t for t in r if t not in ['<sos>', '<eos>', '<pad>']])\n",
        "\n",
        "                # Join tokens into sentences, removing special padding/SOS/EOS tokens\n",
        "                predictions.append(pred_sent)\n",
        "                references.append(ref_sent)\n",
        "\n",
        "    avg_loss = epoch_loss / len(loader)\n",
        "\n",
        "    # Perplexity (PPL) is the exponent of the loss\n",
        "    ppl = math.exp(avg_loss)\n",
        "\n",
        "    # BLEU score using SacreBLEU\n",
        "    bleu = sacrebleu.corpus_bleu(predictions, [references]).score\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"| Test Loss: {avg_loss:.4f}\")\n",
        "    print(f\"| Test Perplexity: {ppl:.4f}\")\n",
        "    print(f\"| BLEU Score: {bleu:.2f}\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "# We ignore the <pad> token index when calculating loss\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=vocab_de.stoi['<pad>'])\n",
        "\n",
        "# Run metrics on your 20% test split\n",
        "get_metrics(model, test_loader, criterion)"
      ],
      "metadata": {
        "id": "YTxvw2apY0ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e2a2292-6fbd-46a1-d24d-39a4ca7f0381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.6.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (6.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "| Test Loss: 3.5367\n",
            "| Test Perplexity: 34.3549\n",
            "| BLEU Score: 26.61\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clean_samples_full(model, loader, vocab_en, vocab_de, n=5):\n",
        "    model.eval()\n",
        "    samples_found = 0\n",
        "\n",
        "    print(\"--- DETAILED SAMPLE TRANSLATIONS ---\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in loader:\n",
        "            # Generate output with Teacher Forcing = 0 (True Model Performance)\n",
        "            output = model(src, trg, 0)\n",
        "\n",
        "            # Convert tensors to token indices\n",
        "            preds = output.argmax(2).transpose(0, 1)\n",
        "            sources = src.transpose(0, 1)\n",
        "            targets = trg.transpose(0, 1)\n",
        "\n",
        "            for i in range(preds.shape[0]):\n",
        "                if samples_found >= n: return\n",
        "\n",
        "                # Decode indices to words\n",
        "                src_toks = vocab_en.decode(sources[i].tolist())\n",
        "                ref_toks = vocab_de.decode(targets[i].tolist())\n",
        "                pred_toks = vocab_de.decode(preds[i].tolist())\n",
        "\n",
        "                # Clean tokens (remove special markers)\n",
        "                src_sent = \" \".join([t for t in src_toks if t not in ['<sos>', '<eos>', '<pad>']])\n",
        "                ref_sent = \" \".join([t for t in ref_toks if t not in ['<sos>', '<eos>', '<pad>']])\n",
        "                pred_sent = \" \".join([t for t in pred_toks if t not in ['<sos>', '<eos>', '<pad>']])\n",
        "\n",
        "                print(f\"\\n[Sample {samples_found + 1}]\")\n",
        "                print(f\"SOURCE (EN):  {src_sent}\")\n",
        "                print(f\"TARGET (DE):  {ref_sent}\")\n",
        "                print(f\"PREDICT (DE): {pred_sent}\")\n",
        "                print(\"-\" * 30)\n",
        "\n",
        "                samples_found += 1\n",
        "\n",
        "# Execute the full sample generator\n",
        "get_clean_samples_full(model, test_loader, vocab_en, vocab_de, n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8SxWsthA0k4",
        "outputId": "c36cf6b7-9bfc-4f03-9b90-dbda4dadbfe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DETAILED SAMPLE TRANSLATIONS ---\n",
            "\n",
            "[Sample 1]\n",
            "SOURCE (EN):  a lady wearing green and white shorts and top is on the beach clapping her hands .\n",
            "TARGET (DE):  eine dame mit grün-weißen shorts und oberteil ist auf dem strand und klatscht in die hände .\n",
            "PREDICT (DE): eine frau in grün und weißen oberteil sitzt am strand und hat sich die hände .\n",
            "------------------------------\n",
            "\n",
            "[Sample 2]\n",
            "SOURCE (EN):  a couple takes their own picture in front of the <unk> <unk> <unk> , from across the street .\n",
            "TARGET (DE):  ein paar macht auf der anderen straßenseite des <unk> <unk> <unk> ein bild von sich .\n",
            "PREDICT (DE): ein paar wartet vor dem <unk> der <unk> der straße der straße der straße .\n",
            "------------------------------\n",
            "\n",
            "[Sample 3]\n",
            "SOURCE (EN):  two women with black dresses and red tops are standing next to a fence smiling .\n",
            "TARGET (DE):  zwei frauen in schwarzen kleidern und roten oberteilen stehen lächelnd neben einem zaun .\n",
            "PREDICT (DE): zwei frauen mit schwarzen kleidern und roten roten stehen neben einem zaun zaun . zaun . zaun . zaun . zaun . zaun . zaun\n",
            "------------------------------\n",
            "\n",
            "[Sample 4]\n",
            "SOURCE (EN):  smiling men wearing numbers pinned to their <unk> race walk down a wet street .\n",
            "TARGET (DE):  lachende männer mit an die brust <unk> nummern laufen eine nasse straße entlang .\n",
            "PREDICT (DE): religiöse männer in regenkleidung schichten kleidung laufen auf eine belebte straße straße straße hinunter .\n",
            "------------------------------\n",
            "\n",
            "[Sample 5]\n",
            "SOURCE (EN):  two old women sitting on curb next to flowers\n",
            "TARGET (DE):  zwei alte frauen sitzen auf dem randstein neben einigen blumen .\n",
            "PREDICT (DE): zwei alte frauen sitzen auf steinstufen neben kisten .\n",
            "------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}